{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"VRUI Overview","text":"<p>VRUI is both a collection of software and a community of XR developers and scientific investigators and educators who use XR for research and teaching.</p> <p>The Vrui Toolkit is a comprehensive development framework designed to facilitate the creation of immersive virtual reality applications. It provides a suite of tools and libraries that enable developers to build VR software with advanced 3D user interfaces, supporting a wide range of input devices and display systems. Vrui emphasizes flexibility and scalability, making it suitable for applications in scientific visualization, data exploration, and other fields requiring interactive 3D environments.</p> <p>VRUI Applications are applications developed using the Developer Toolkit and deployed as part of VRUI installation to provide specific XR experiences. For example, the LIDAR Viewer is an interactive application for processing, visualization, and analysis of large 3D point cloud data produced by terrestrial or airborne LiDAR scanning. Links to the repositories of VRUI applications that are released as Open Source Software (OSS) and whose developer(s) commit to upholding the VRUI Code of Conduct appear in VRUI Organizations lit of repositories on this page.</p> <p>The VRUI Community is the entire community of VRUI users and developers. (As of March, 2025, we are aware of approximately 2,000 VRUI users). Please see our Code of Conduct for detailed information about VRUI values and standards.</p>"},{"location":"arsandbox/","title":"Augmented Reality Sandbox","text":"<p>The Augmented Reality Sandbox is an augmented reality application that scans a sand surface using a Kinect 3D camera and projects a topography map that is updated in real time with topographic contour lines, hillshading, and an optional real-time water flow simulation back onto the sand surface using a calibrated projector.</p> <p>The ARSandbox package contains the sandbox application itself, <code>SARndbox</code>, and a calibration utility to interactively measure a transformation between the Kinect camera scanning the sandbox surface, and the projector projecting onto it.</p> <p>Documentation: vrui-vr.github.io/docs/arsandbox</p> <p>Source code: github.com/vrui-vr/arsandbox</p>"},{"location":"arsandbox/in_action/","title":"Check out the ARSandbox in action!","text":""},{"location":"arsandbox/in_action/#introductory-examples","title":"Introductory examples","text":"<p>The short video playlist embedded below demonstrates the use of contour lines, elevation color mapping, and basic topography in the ARSandbox.</p>"},{"location":"arsandbox/in_action/#advanced-examples","title":"Advanced examples","text":""},{"location":"arsandbox/in_action/#simulating-water-flow","title":"Simulating water flow","text":""},{"location":"arsandbox/in_action/#simulating-dam-failure","title":"Simulating dam failure","text":""},{"location":"arsandbox/in_action/#more","title":"More!","text":"<p>More examples of the ARSandbox in action can be found at youtube.com/@okreylos. </p>"},{"location":"arsandbox/installation/","title":"Getting started with the ARSandbox","text":"<p>Heads up!</p> <p>Both VRUI and the Kinect package must be installed before the ARSandbox can be installed.</p> <ol> <li>Install VRUI (go to instructions \\(\\rightarrow\\))</li> <li>Install Kinect package (go to instructions \\(\\rightarrow\\))</li> <li>Install ARSandbox<ol> <li>Setup and calibrate the hardware</li> <li>Install the software</li> </ol> </li> </ol>"},{"location":"arsandbox/installation/hardware/","title":"Setup and calibration","text":"<p>Before the Augmented Reality Sandbox can be used, the hardware (physical sandbox, Kinect camera, and projector) has to be set up properly, and the various components have to be calibrated internally and with respect to each other. While the sandbox can be run in \"trial mode\" with very little required setup, for the full effect the following steps have to be performed in order:</p> <ol> <li> <p>(Optional) Calculate per-pixel depth correction coefficients for the Kinect camera.</p> </li> <li> <p>(Optional) Internally calibrate the Kinect camera. We strongly recommend skipping this step on initial installation, and only performing it if there are intolerable offsets between the real sand surface in the AR Sandbox and the projected topographic image.</p> </li> <li> <p>Mount the Kinect camera above the sandbox so that it is looking straight down, and can see the entire sand surface. Use <code>RawKinectViewer</code> from the Kinect 3D video capture project to line up the depth camera while ignoring the color camera.</p> </li> <li> <p>Measure the base plane equation of the sand surface relative to the Kinect camera's internal coordinate system using <code>RawKinectViewer</code>'s plane extraction tool. (See \"Using Vrui Applications\" in the Vrui HTML documentation on how to use <code>RawKinectViewer</code>, and particularly on how to create/destroy tools.)</p> </li> <li> <p>Measure the extents of the sand surface relative to the Kinect camera's internal coordinate system using KinectViewer and a 3D measurement tool.</p> </li> <li> <p>Mount the projector above the sand surface so that it projects its image perpendicularly onto the flattened sand surface, and so that the projector's field-of-projection and the Kinect camera's field-of- view overlap as much as possible. Focus the projector to the flattened average-height sand surface.</p> </li> <li> <p>Calculate a calibration matrix from the Kinect camera's camera space to projector space using the CalibrateProjector utility and a circular calibration target (a CD with a fitting white paper disk glued to one surface).</p> </li> <li> <p>Test the setup by running the Augmented Reality Sandbox application.</p> </li> </ol>"},{"location":"arsandbox/installation/hardware/note_on_water_simulation/","title":"A note on water simulation","text":"<p>Without the real-time water simulation, the Augmented Reality Sandbox has very reasonable hardware requirements. Any current PC with any current graphics card should be able to run it smoothly.</p> <p>The water simulation, on the other hand, places extreme load even on high-end current hardware. We therefore recommend to turn off the water simulation, using the <code>-ws 0.0 0</code> command line option to SARndbox, or reducing its resolution using the <code>-wts &lt;width&gt; &lt;height&gt;</code> command line option with small sizes, e.g., <code>-wts 200 150</code>, for initial testing or unless the PC running the Augmented Reality Sandbox has a top-of-the line CPU, a high-end gaming graphics card, e.g., an Nvidia GeForce 970 and the vendor-supplied proprietary drivers for that graphics card.</p>"},{"location":"arsandbox/installation/hardware/step1/","title":"Step 1: Per-pixel depth correction (optional)","text":"<p>Kinect cameras have non-linear distortions in their depth measurements due to uncorrected lens distortions in the depth camera. The Kinect 3D video capture project has a calibration tool to gather per-pixel correction factors to \"straighten out\" the depth image.</p> <p>To calculate depth correction coefficients, start the <code>RawKinectViewer</code> utility and create a <code>Calibrate Depth Lens</code> tool (See \"Using Vrui Applications\" in the Vrui HTML documentation on how to create tools).</p> <p>Then, find a completely flat surface and point the Kinect camera perpendicularly at that surface from a variety of distances. Ensure that the depth camera only sees the flat surface and no other objects and that there are no holes in the depth images.</p> <p>Now, capture one depth correction tie point for each distance between the Kinect camera and the flat surface:</p> <ol> <li> <p>Line up the Kinect camera.</p> </li> <li> <p>Capture an average depth frame by selecting the \"Average Frames\" main menu item, and wait until a static depth frame is displayed.</p> </li> <li> <p>Create a tie point by pressing the first button bound to the <code>Calibrate Depth Lens</code> tool.</p> </li> <li> <p>De-select the \"Average Frames\" main menu item, and repeat from step 1 until the surface has been captured from sufficiently many distances.</p> <p>After all tie points have been collected:</p> </li> <li> <p>Press the second button bound to the <code>Calibrate Depth Lens</code> tool to calculate the per-pixel depth correction factors based on the collected tie points. This will write a depth correction file to the Kinect 3D video capture project's configuration directory, and print a status message to the terminal.</p> </li> </ol>"},{"location":"arsandbox/installation/hardware/step2/","title":"Step 2: Internally calibrate the Kinect camera (optional)","text":""},{"location":"arsandbox/installation/hardware/step2/#background-on-kinect-camera-calibration","title":"Background on Kinect camera calibration","text":"<p>Individual Kinect cameras have slightly different internal layouts and slightly different optical properties, meaning that their internal calibrations, i.e., the projection matrices defining how to project depth images back out into 3D space and how to project color images onto those reprojected depth images, differ individually as well. While all Kinects are factory-calibrated and contain the necessary calibration data in their firmware, the format of those data is proprietary and cannot be read by the Kinect 3D video capture project software, meaning that each Kinect camera has to be calibrated internally before it can be used. In practice, the differences are small and a Kinect camera can be used without internal calibration by assigning default calibration values, but it is strongly recommended to perform calibration on each device individually.</p> <p>The internal calibration procedure requires a semi-transparent calibration target; precisely, a checkerboard with alternating clear and opaque tiles. Such a target can be constructed by gluing a large sheet of paper to a clear glass plate, drawing or ideally printing a checkerboard onto it, and cutting out all \"odd\" tiles using large rulers and a sharp knife. It is important that the tiles are lined up precisely and have precise sizes, and that the clear tiles are completely clean without any dust, specks, or fingerprints. Calibration targets can have a range of sizes and numbers of tiles, but we found the ideal target to contain 7 \\(\\times\\) 5 tiles of 3.5in \\(\\times\\) 3.5in each.</p> <p>Given an appropriate calibration target, the calibration process is performed using RawKinectViewer and its <code>Draw Grids</code> tool. The procedure is to show the calibration target to the Kinect camera from a variety of angles and distances, and to capture a calibration tie point for each viewpoint by fitting a grid to the target's images in the depth and color streams interactively.</p>"},{"location":"arsandbox/installation/hardware/step2/#detailed-calibration-procedure","title":"Detailed calibration procedure","text":"<ol> <li> <p>Aim Kinect camera at calibration target from a certain position and angle. It is important to include several views where the calibration target is seen at an angle.</p> </li> <li> <p>Capture an average depth frame by selecting the \"Average Frames\" main menu item, and wait until a static depth frame is displayed.</p> </li> <li> <p>Drag the virtual grids displayed in the depth and color frames using the <code>Draw Grid</code> tool's first button until the virtual grids exactly match the calibration target. Matching the target in the depth frame is relatively tricky due to the inherent fuzziness of the Kinect's depth camera. Doing this properly will probably take some practice. The important idea is to get a \"best fit\" between the calibration target and the grid. For the particular purpose of the Augmented Reality Sandbox, the color frame grids can be completely ignored because only the depth camera is used; however, since calibration files are shared between all uses of the Kinect 3D video capture project, it is best to perform a full, depth and color, calibration.</p> </li> <li> <p>Press the <code>Draw Grid</code> tool's second button to store the just-created calibration tie point.</p> </li> <li> <p>Deselect the \"Average Frames\" main menu entry, and repeat from step 1 until a sufficient number of calibration tie points have been captured. The set of all tie points already selected can be displayed by pressing the <code>Draw Grid</code> tool's third button.</p> <p>After all tie points have been collected:</p> </li> <li> <p>Press the <code>Draw Grid</code> tool's fourth button to calculate the Kinect camera's internal calibration parameters. These will be written to an intrinsic parameter file in the Kinect 3D video capture project's configuration directory.</p> </li> </ol>"},{"location":"arsandbox/installation/hardware/step2/#tutorial-video","title":"Tutorial video","text":"<p>This calibration step is illustrated in the following tutorial video:</p>"},{"location":"arsandbox/installation/hardware/step3/","title":"Step 3: Mount the Kinect camera above the sandbox","text":"<p>In theory, the Kinect camera can be aimed at the sand surface from any position and/or angle.</p> <p>For best results, we recommend to:</p> <ul> <li>position the camera such that it looks straight down onto the surface,</li> <li>position the camera such that the depth camera's field-of-view exactly matches the extents of the sandbox.</li> </ul> <p><code>RawKinectViewer</code> can be used to get real-time visual feedback while aligning the Kinect camera.</p>"},{"location":"arsandbox/installation/hardware/step4/","title":"Step 4: Measure the base plane equation of the sand surface","text":""},{"location":"arsandbox/installation/hardware/step4/#measuring-the-base-plane","title":"Measuring the base plane","text":"<p>Because the Kinect camera can be aimed at the sand surface arbitrarily, the Augmented Reality Sandbox needs to know the equation of the \"base plane\" corresponding to the average flattened sand surface and the \"up direction\" defining elevation (+/-) relative to that base plane.</p> <p>The base plane can be measured using <code>RawKinectViewer</code> and the <code>Extract Planes</code> tool.</p> <ol> <li>Flatten and average the sand surface such that it is exactly horizontal, or place a flat board above the sand surface.</li> <li>Then, capture an average depth frame by selecting the \"Average Frames\" main menu entry and wait until the depth image stabilizes.</li> <li>Now, use the <code>Extract Planes</code> tool to draw a rectangle in the depth frame that only contains the flattened sand surface. After releasing the <code>Extract Planes</code> tool's button, the tool will calculate the equation of the plane best fitting the selected depth pixels, and print two versions of that plane equation to the terminal: the equation in depth image space, and the equation in camera space. Of these, only the second is important. The tool prints the camera-space plane equation in the form</li> </ol> \\[ x * (\\text{normal}_x, \\text{normal}_y, \\text{normal}_z) = \\text{offset} \\] <p>This equation must be entered into the sandbox layout file, which is by default called <code>BoxLayout.txt</code> and contained in the Augmented Reality Sandbox's configuration directory. The format of this file is simple; the first line contains the sandbox's base plane equation in the form:</p> \\[ (\\text{normal}_x, \\text{normal}_y, \\text{normal}_z), \\text{offset} \\] <p>The plane equation printed by the <code>Extract Planes</code> tool only needs to be modified slightly when pasting it into the sandbox layout file: the \"\\(x\\,*\\)\" part has to be removed and the equal sign has to be replaced by a comma. The other four lines in the sandbox layout file are filled in in the next calibration step.</p>"},{"location":"arsandbox/installation/hardware/step4/#base-plane-and-zero-elevation-level","title":"Base plane and zero elevation level","text":"<p>The base plane equation defines the zero elevation level of the sand surface. Since standard color maps equate zero elevation with sea level, and due to practical reasons, the base plane is often measured above the flattened average sand surface, it might be desirable to lower the zero elevation level. This can be done easily be editing the sandbox layout file.</p> <p>The zero elevation level can be shifted upwards by increasing the offset value (the fourth component) of the plane equation, and can be shifted downwards by decreasing the offset value. The offset value is measured in cm; therefore, adding 10 to the original offset value will move sea level 10 cm upwards.</p>"},{"location":"arsandbox/installation/hardware/step4/#tutorial-video","title":"Tutorial video","text":"<p>This calibration step is illustrated in the following tutorial video:</p>"},{"location":"arsandbox/installation/hardware/step5/","title":"Step 5: Measure the extents of the sand surface","text":"<p>The Augmented Reality Sandbox needs to know the lateral extents of the visible sand surface with respect to the base plane. These are defined by measuring the 3D positions of the four corners of the flattened average sand surface using <code>RawKinectViewer</code> and a 3D measurement tool, and then entering those positions into the sandbox layout file.</p> <p>Start <code>RawKinectViewer</code> and create a 3D measurement tool by assigning a <code>Measure 3D Positions</code> tool to some button. Then measure the 3D positions of the four corners of the flattened sand surface in the order lower left, lower right, upper left, upper right; in other words, form a mirrored Z starting in the lower left.</p> <p>To measure a 3D position, press and release the button to which the measurement tool was bound inside the depth frame. This will query the current depth value at the selected position, project it into 3D camera space, and print the resulting 3D position to the console. Simply paste the four corner positions, in the order mentioned above, into the sandbox layout file.</p>"},{"location":"arsandbox/installation/hardware/step6/","title":"Step 6: Mount the projector above the sandbox","text":"<p>Just like with the Kinect camera, the Augmented Reality Sandbox is capable of dealing with arbitrary projector alignments. As long as there is some overlap between the Kinect camera's field-of-view and the projector's projection area, the two can be calibrated with respect to each other. However, for several reasons, it is best to align the projector carefully such that it projects perpendicularly to the flattened average sand surface. The main reason is pixel distortion: if the projection is wildly off-axis, the size of projected pixels will change sometimes drastically along the sand surface. While the Augmented Reality Sandbox can account for overall geometric distortion, it cannot change the size of displayed pixels, and the projected image looks best if all pixels are approximately square and the same size.</p> <p>Some projectors, especially short-throw projectors, have off-axis projections, meaning that the image is not centered on a line coming straight out of the projection lens. In such cases, perpendicular projection does not imply that the projector is laterally centered above the sandbox; in fact, it will have to be mounted off to one side. The criterion to judge perpendicular projection is that the projected image appears as a rectangle, not a trapezoid.</p> <p>We strongly recommend against using any built-in keystone correction a particular projector model might provide. The Augmented Reality Sandbox corrects for keystoning internally, and projector-based keystone correction works on an already pixelated image, meaning that it severely degrades image quality. Never use keystone correction. Align the projector as perpendicularly as possible, and let the Augmented Reality Sandbox handle the rest.</p> <p>The second reason to aim for perpendicular projections is focus. Projector images are focused in a plane perpendicular to the projection direction, meaning that only a single line of the projected image will be in correct focus when a non-perpendicular projection is chosen. Either way, after the projector has been mounted, we recommend to focus it such that the entirety of the flattened average sand surface is as much in focus as possible.</p> <p>Heads up!</p> <p>On a tangential note, we also strongly recommend to only run projectors at their native pixel resolutions. Most projector models will support a wide range of input video formats to accommodate multiple uses, but using any resolution besides the one corresponding to the projector's image generator is a very bad idea because the projector will have to rescale the input pixel grid to its native pixel grid, which causes severe degradation in image quality. Some projectors \"lie\" about their capabilities to seem more advanced, resulting in a suboptimal resolution when using plug &amp; play or automatic setups. It is always a good idea to check the projector's specification for its native resolution and ensure that the graphics card uses that resolution when the projector is connected.</p>"},{"location":"arsandbox/installation/hardware/step7/","title":"Step 7: Calculate the projector calibration matrix","text":"<p>The most important step to create a true augmented reality display is to calibrate the Kinect camera capturing the sand surface and the projector projecting onto it with respect to each other, so that the projected colors and topographic contour lines appear exactly in the right place. Without this calibration, the Augmented Reality Sandbox is just a sandbox with some projection.</p> <p>This calibration step is performed using the <code>CalibrateProjector</code> utility, and a custom calibration target. This target has to be a flat circular disk whose exact center point is marked in some fashion. We recommend to use an old CD, glue a white paper disk of the proper size to one side, and draw two orthogonal lines through the CD's center point onto the paper disk. It is important that the two lines intersect in the exact center of the disk.</p> <p>The calibration procedure is to place the disk target into the Kinect camera's field-of-view in a sequence of prescribed positions, guided by the projector. When <code>CalibrateProjector</code> is started, it will first capture a background image of the current sand surface; it is important that the surface is not disturbed during or after this capture step, and that no other objects are between the Kinect camera and the sand surface. Afterwards, <code>CalibrateProjector</code> will collect a sequence of 3D tie points. For each tie point, it will display two intersecting lines. The user has to position the disk target such that the projected lines exactly intersect in the disk's center point, and such that the disk surface is parallel to the flattened average sand surface, i.e., the base plane that was collected in a previous calibration step. It is important to place the disk at a variety of elevations above and ideally below the base surface to collect a full 3D calibration matrix. If all tie points are in the same plane, the calibration procedure will fail.</p>"},{"location":"arsandbox/installation/hardware/step7/#detailed-calibration-procedure","title":"Detailed calibration procedure","text":"<ol> <li> <p>Start <code>CalibrateProjector</code> and wait for it to collect a background frame. Background capture is active while the screen is red. It is essential to run <code>CalibrateProjector</code> in full-screen mode on the projector, or the resulting calibration will be defective. See the Vrui user's manual on how to force Vrui applications to run at the proper position and size. Alternatively, switch <code>CalibrateProjector</code> into full-screen mode manually by pressing the F11 function key. When started, <code>CalibrateProjector</code> must be told the exact pixel size of the projector's image using the <code>-s &lt;width&gt; &lt;height&gt;</code> command line option. Using a wrong pixel size will result in a defective calibration. The recommended BenQ short-throw projector has 1024x768 pixels, which is also the default in the software. In other words, when using an XGA-resolution projector, the <code>-s</code> option is not required.</p> </li> <li> <p>Create a <code>Capture</code> tool and bind it to two keys (here \"1\" and \"2\"). Press and hold \"1\" and move the mouse to highlight the <code>Capture</code> item in the tool selection menu that pops up. Then release \"1\" to select the highlighted item. This will open a dialog box prompting to press a second key; press and release \"2\". This will close the dialog box. Do not press \"1\" again when the dialog box is still open; that will cancel the tool creation process. This process binds functions to two keys: \"1\" will capture a tie point, and \"2\" will re-capture the background sand surface. \"2\" should only be pressed if the sand surface changes during the calibration procedure, for example if a hole is dug to capture a lower tie point. After any change to the sand surface, remove the calibration object and any other objects, press \"2\", and wait for the screen to turn black again.</p> </li> <li> <p>Place the disk target at some random elevation above or below the flattened average sand surface such that the intersection of the projected white lines exactly coincides with the target's center point.</p> </li> <li> <p>Remove your hands from the disk target and confirm that the target is seen by the Kinect camera. <code>CalibrateProjector</code> will display all non-background objects as yellow blobs, and the object it identified as the calibration target as a green blob. Because there is no calibration yet, the green blob corresponding to the disk target will not be aligned with the target; simply ensure that there is a green blob, that it is circular and stable, and that it matches the actual calibration target (put your hand next to it, and see if the yellow blob matching your hand appears next to the green blob).</p> </li> <li> <p>Press the <code>Capture</code> tool's first button (\"1\"), and wait until the tie point is captured. Do not move the calibration target or hold any objects above the sand surface while a tie point is captured.</p> </li> <li> <p><code>CalibrateProjector</code> will move on to the next tie point position, and display a new set of white lines. Repeat from step 3 until all tie points have been captured. Once the full set has been collected, <code>CalibrateProjector</code> will calculate the resulting calibration matrix, print some status information, and write the matrix to a file inside the Augmented Reality Sandbox's configuration directory. The user can continue to capture more tie points to improve calibration as desired; the calibration file will be updated after every additional tie point. Simply close the application window when satisfied. Additionally, after the first round of tie points has been collected, <code>CalibrateProjector</code> will track the calibration target in real-time and indicate its position with red crosshairs. To check calibration quality, place the target anywhere in or above the sandbox, remove your hands, and ensure that the red crosshairs intersect in the target's center.</p> </li> </ol>"},{"location":"arsandbox/installation/hardware/step8/","title":"Step 8: Run the Augmented Reality Sandbox","text":"<p>At this point, calibration is complete. It is now possible to run the main Augmented Reality Sandbox application from inside the source code directory (or the -- optionally chosen -- installation directory):</p> <pre><code>./bin/SARndbox -uhm -fpv\n</code></pre> <p>The <code>-fpv</code> (\"fix projector view\") option tells the AR Sandbox to use the projector calibration matrix created in step 7. The <code>-uhm</code> (\"use height map\") option tells the AR Sandbox to color-map the 3D surface by elevation, using the default height color map.</p> <p>It is very important to run the application in full-screen mode on the projector, or at least with the exact same window position and size as <code>CalibrateProjector</code> in step 7. If this is not done correctly, the calibration will not work as desired. To manually switch <code>SARndbox</code> into full-screen mode after start-up, press the F11 function key.</p> <p>To check the calibration, observe how the projected colors and topographic contour lines exactly match the physical features of the sand surface. If there are discrepancies between the two, repeat calibration step 7 until satisfied. On a typical 40in \\(\\times\\) 30in sandbox, where the Kinect is mounted approximately 38in above the sandbox's center point, and using a perpendicularly projecting 1024\\(\\times\\)768 projector, alignment between the real sand surface and the projected features should be on the order of 1 mm.</p> <p><code>SARndbox</code> provides a plethora of configuration files and command line options to fine-tune the operation of the Augmented Reality Sandbox as desired. Run <code>SARndbox -h</code> to see the full list of options and their default values, or refer to external documentation on the project's web site.</p>"},{"location":"arsandbox/installation/software/","title":"Installing the Augmented Reality Sandbox","text":"<p>Both the simple and advanced instructions will install the ARSandbox with the standard settings.</p> <p>The Augmented Reality Sandbox package contains the sandbox application itself, <code>SARndbox</code>, and a calibration utility to interactively measure a transformation between the Kinect camera scanning the sandbox surface, and the projector projecting onto it. The setup procedure described below also uses several utilities from the Kinect 3D video capture project.</p> <p>Warning</p> <p>The Augmented Reality Sandbox requires Vrui version 13.0 build 001 or newer, and the Kinect 3D Video Capture Project version 5.0 or newer.</p>"},{"location":"arsandbox/installation/software/#prerequisites","title":"Prerequisites","text":"<p>The Augmented Reality Sandbox requires Vrui version 13.0 build 001 or newer, and the Kinect 3D Video Capture Project version 5.0 or newer.</p> <p>Both the simple and advanced installation instructions assume that you have already installed the Vrui VR Toolkit and the Kinect 3D Video Package by following the simple installation instructions of those respective packages.</p>"},{"location":"arsandbox/installation/software/#simple-vs-advanced-install","title":"Simple vs. advanced install","text":"<p>If you want to run the ARSandbox from multiple user accounts, or from a dedicated user account with limited access rights, you need to install it in a system location where it can be accessed by those user accounts.</p> <p>For example, you could install it in the same <code>/usr/local</code> directory hierarchy where you installed the Vrui VR Toolkit following the simple instructions included with Vrui.</p> <p>To do this, follow the advanced install instructions.</p>"},{"location":"arsandbox/installation/software/advanced_install/","title":"Advanced Augmented Reality Sandbox Installation guide","text":"<p>First, you will need to modify the build command shown in Step 2.</p> <p>Start by navigating to the ARSandbox directory:</p> <pre><code>cd &lt;AR Sandbox directory&gt;\n</code></pre> <p>replacing <code>&lt;AR Sandbox directory&gt;</code> with the actual directory path as usual.</p> <p>If you already built the ARSandbox according to Step 2, you first have to update its configuration. Enter into the same terminal window:</p> <pre><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt; INSTALLDIR=&lt;installation location&gt; config\n</code></pre> <p>replacing <code>&lt;installation location&gt;</code> with your chosen location, for example <code>INSTALLDIR=/usr/local</code>.</p> <p>Then, build the ARSandbox for installation in your chosen destination by entering:</p> <pre><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt; INSTALLDIR=&lt;installation location&gt;\n</code></pre> <p>replacing <code>&lt;installation location&gt;</code> with the same location as in the previous command. You can add <code>-j&lt;number of cpus&gt;</code> to optimize the build process, as described in Step 2.</p> <p>Finally, install the ARSandbox in your chosen location by entering:</p> <pre><code>sudo make VRUI_MAKEDIR=&lt;Vrui build system location&gt; INSTALLDIR=&lt;installation location&gt; install\n</code></pre> <p>again using the same <code>&lt;installation location&gt;</code>.</p>"},{"location":"arsandbox/installation/software/simple_install/","title":"Simple Augmented Reality Sandbox Installation guide","text":"Heads up! <p>Angle brackets <code>&lt;&gt;</code> in commands below are placeholders, meaning that you have to replace everything between, and including, the angle brackets with some text that depends on your specific circumstances.</p> <p>For example, if your host has eight CPUs, instead of entering <code>-j&lt;number of CPUs&gt;</code> as part of some command, you would enter <code>-j8</code>.</p>"},{"location":"arsandbox/installation/software/simple_install/#step-1-download-the-arsandbox-repository-from-github","title":"Step 1: Download the ARSandbox repository from GitHub","text":"<p>The ARSandbox code repository can be downloaded either by:</p> <ol> <li>downloading the zip file and unpacking it OR</li> <li>cloning the repository with <code>git clone</code></li> </ol> <p>Warning</p> <p>If you are unfamiliar with git and/or GitHub, you should probably go the zip file route.</p>"},{"location":"arsandbox/installation/software/simple_install/#option-1-downloading-and-unpacking-a-zip-file-from-github","title":"Option 1: Downloading and unpacking a zip file from GitHub","text":"<p>On the ARSandbox repository's main page, click on the green \"&lt;&gt; Code\" button, and then click on \"Download ZIP\" in the menu that pops up in response.</p> <p></p> <p>Depending on your browser settings, you may be asked where to store the file being downloaded, or it might be stored in a default location, such as your <code>Downloads</code> directory. Take note of what the zip file is called and where it is stored.</p> <p>Assuming that you already created the <code>src</code> directory according to Vrui's installation instructions, enter the following line into a terminal window once the file is completely downloaded:</p> <pre><code>cd ~/src\n</code></pre> <p>Then enter into the same terminal window:</p> <pre><code>unzip &lt;path to downloaded zip file&gt;\n</code></pre> <p>Replace <code>&lt;path to downloaded zip file&gt;</code> with the full path to the zip file, for example <code>~/Downloads/arsandbox-main.zip</code>.</p> <p>Finally, check for the name of your new ARSandbox directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>arsandbox-main</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;ARSandbox directory&gt;\n</code></pre> <p>where you replace <code>&lt;ARSandbox directory&gt;</code> with the name of the directory where you cloned/unpacked the ARSandbox in the previous step, as printed by <code>ls</code>.</p>"},{"location":"arsandbox/installation/software/simple_install/#option-2-clone-the-repository-from-github","title":"Option 2: Clone the repository from GitHub","text":"<p>Assuming that you already created the <code>src</code> directory according to Vrui's installation instructions, navigate to the <code>src</code> directory on your computer in the terminal window.</p> <pre><code>cd ~/src\n</code></pre> <p>Then, clone the repository from GitHub:</p> <pre><code>git clone https://github.com/vrui-vr/arsandbox.git\n</code></pre> <p>Finally, check for the name of your new ARSandbox directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>arsandbox</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;ARSandbox directory&gt;\n</code></pre> <p>where you replace <code>&lt;ARSandbox directory&gt;</code> with the name of the directory where you cloned/unpacked the ARSandbox in the previous step, as printed by <code>ls</code>.</p>"},{"location":"arsandbox/installation/software/simple_install/#step-2-build-the-arsandbox","title":"Step 2: Build the ARSandbox","text":"<p>Heads up!</p> <p>Make sure you are in the new <code>arsandbox-main</code> (from option 1) or <code>arsandbox</code> (from option 2) directory.</p> <p>To build the ARSandbox, enter into the same terminal window:</p> <pre><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt;\n</code></pre> <p>where you replace <code>&lt;Vrui build system location&gt;</code> with the location of Vrui's build system on your host, as described in Vrui's installation instructions.</p> <p>Example</p> <p>Your command will look something like this:</p> <p><code>make VRUI_MAKEDIR=/usr/local/share/Vrui-13.1/make</code></p> Tip <p>You can speed up the build process if your host has multiple CPUs or CPU cores. Instead of the above, enter into the same terminal:</p> <p><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt; -j&lt;number of cpus&gt;</code></p> <p>again replacing <code>&lt;Vrui build system location&gt;</code> with the location of Vrui's build system on your host, and replacing <code>&lt;number of cpus&gt;</code> with the number of CPUs or CPU cores on your host, say <code>-j8</code> if you have eight cores. Note that there is no space between the <code>-j</code> and the number of cores.</p> <p>Using <code>-j$(nproc)</code> (exactly as written) will tell your computer to figure out how many cores it has.</p> <p>Once <code>make</code> has finished running, check that there were no error messages. The quickest way to check whether the ARSandbox built successfully is to run the <code>make</code> command a second time, exactly as you entered it the first time.</p> <p>If everything went well the first time, the second run will print:</p> <pre><code>make: Nothing to be done for 'all'.\n</code></pre>"},{"location":"arsandbox/installation/software/simple_install/#step-3-installing-the-arsandbox","title":"Step 3: Installing the ARSandbox","text":"<p>If built following these simple instructions, the ARSandbox does not need to be installed. You can run the built applications, <code>CalibrateProjector</code>, <code>SARndbox</code>, and <code>SARndboxClient</code>, directly from the directory where you cloned or unpacked the sources.</p> <p>For example, to run the main ARSandbox application, you would enter the following into a terminal window:</p> <pre><code>cd &lt;ARSandbox directory&gt;\n./bin/SARndbox\n</code></pre> <p>where you replace <code>&lt;ARSandbox directory&gt;</code> with the full name of the directory where you cloned/unpacked the ARSandbox sources, for example:</p> <pre><code>cd ~/src/arsandbox-main\n./bin/SARndbox\n</code></pre>"},{"location":"kinect/","title":"VRUI Kinect package","text":""},{"location":"kinect/installation/","title":"Getting started with the Kinect package","text":"<p>Heads up!</p> <p>VRUI must be installed before the Kinect package can be installed.</p> <ol> <li>Install VRUI (go to instructions \\(\\rightarrow\\))</li> <li>Install Kinect package!!!</li> </ol>"},{"location":"kinect/installation/#prerequisites","title":"Prerequisites","text":"<p>This software requires the Vrui VR toolkit, version 13.0 build 001 or newer. It also requires <code>libusb</code> version 1.0, and that Vrui was configured with support for <code>libusb</code> prior to its installation. To properly work with multiple Kinect-for-Xbox version 1473 and/or Kinect-for-Windows devices, the <code>libusb</code> library needs to provide USB bus topology query functions such as those provided by the <code>libusbx</code> fork. This software can optionally create Kinect 3D video streaming plug-ins for Vrui's collaboration infrastructure (version 2.9 or newer). The presence of Vrui's collaboration infrastructure will be detected automatically during the build process.</p>"},{"location":"kinect/installation/guide/","title":"Installation guide","text":"<p>Tip</p> <p>It is recommended to download or move the source packages for Vrui and the Kinect 3D Video Capture Project into a <code>src</code> directory underneath the user's home directory. Otherwise, references to <code>~/src</code> in the following instructions need to be changed.</p> <p>More info about the collaboration infrastructure and the installation guide can be found at github.com/vrui-vr/collaboration/blob/main/README. --&gt;</p>"},{"location":"kinect/installation/guide/#step-1-unpack-the-kinect-3d-video-capture","title":"Step 1: Unpack the Kinect 3D Video Capture","text":""},{"location":"kinect/installation/guide/#option-1-downloading-and-unpacking-a-zip-file-from-github","title":"Option 1: Downloading and unpacking a zip file from GitHub","text":"<p>On the Kinect repository's main page, click on the green \"&lt;&gt; Code\" button, and then click on \"Download ZIP\" in the menu that pops up in response.</p> <p></p> <p>Depending on your browser settings, you may be asked where to store the file being downloaded, or it might be stored in a default location, such as your <code>Downloads</code> directory. Take note of what the zip file is called and where it is stored.</p> <p>Assuming that you already created the <code>src</code> directory according to Vrui's installation instructions, enter the following line into a terminal window once the file is completely downloaded:</p> <pre><code>cd ~/src\n</code></pre> <p>Then enter into the same terminal window:</p> <pre><code>unzip &lt;path to downloaded zip file&gt;\n</code></pre> <p>Replace <code>&lt;path to downloaded zip file&gt;</code> with the full path to the zip file, for example <code>~/Downloads/kinect-main.zip</code>.</p> <p>Finally, check for the name of your new Kinect directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>kinect-main</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;Kinect directory&gt;\n</code></pre> <p>where you replace <code>&lt;Kinect directory&gt;</code> with the name of the directory where you cloned/unpacked the Kinect in the previous step, as printed by <code>ls</code>.</p>"},{"location":"kinect/installation/guide/#option-2-clone-the-repository-from-github","title":"Option 2: Clone the repository from GitHub","text":"<p>Assuming that you already created the <code>src</code> directory according to Vrui's installation instructions, navigate to the <code>src</code> directory on your computer in the terminal window.</p> <pre><code>cd ~/src\n</code></pre> <p>Then, clone the repository from GitHub:</p> <pre><code>git clone https://github.com/vrui-vr/kinect.git\n</code></pre> <p>Finally, check for the name of your new Kinect directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>kinect</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;Kinect directory&gt;\n</code></pre> <p>where you replace <code>&lt;Kinect directory&gt;</code> with the name of the directory where you cloned/unpacked the Kinect in the previous step, as printed by <code>ls</code>.</p> <p>Warning</p> <p>If your installed Vrui version is not 13.0, or Vrui's installation directory was changed from the default of <code>/usr/local</code>, adapt the makefile using a text editor.</p> <p>Change the value of <code>VRUI_MAKEDIR</code> close to the beginning of the file as follows: <code>VRUI_MAKEDIR := &lt;Vrui install dir&gt;/share/make</code>, where  is the installation directory chosen when you installed Vrui. Use <code>$(HOME)</code> to refer to the user's home directory instead of <code>~</code>."},{"location":"kinect/installation/guide/#step-2-build-the-kinect-3d-video-capture-project","title":"Step 2: Build the Kinect 3D Video Capture Project","text":"<p>Run the following from inside the <code>&lt;Kinect directory&gt;</code>:</p> <pre><code>make\n</code></pre> <p>Once <code>make</code> has finished running, check that there were no error messages. The quickest way to check whether the Kinect built successfully is to run the <code>make</code> command a second time, exactly as you entered it the first time.</p> <p>If everything went well the first time, the second run will print:</p> <pre><code>make: Nothing to be done for 'all'.\n</code></pre>"},{"location":"kinect/installation/guide/#step-3-install-the-kinect-3d-video-capture-project","title":"Step 3: Install the Kinect 3D Video Capture Project","text":"<p>If Vrui was installed in <code>/usr/local</code> or elsewhere outside the user's home directory:</p> <pre><code>sudo make install\n</code></pre> <p>This will ask for the user's password to install the Kinect package inside Vrui's installation directory.</p> <p>If Vrui was installed inside the user's home directory:</p> <pre><code>make install\n</code></pre>"},{"location":"kinect/installation/guide/#step-4-install-a-udev-rule-file-to-give-access-to-all-users-optional-linux-only","title":"Step 4: Install a udev rule file to give access to all users (optional, Linux-only)","text":"<p>By default, Kinect devices can only be accessed by the root user. This is inconvenient and a security risk, as all Kinect applications must be run as root.</p> <p>The Kinect package contains a <code>udev</code> rule file to give full access to any Kinect device to the user currently logged in to the local console.</p> <p>First, run:</p> <pre><code>sudo make installudevrules\n</code></pre> <p>This will ask for the user's password to copy the rule file, <code>69-Kinect.rules</code>, into the <code>udev</code> rule directory <code>/etc/udev/rules.d</code>. After the rule file has been installed, it needs to be activated.</p> <p>The recommended way is to restart the device manager:</p> <pre><code>sudo udevadm control --reload\nsudo udevadm trigger --action=change\n</code></pre> <p>Alternatively, the rule can be activated by unplugging the Kinect from its USB port, waiting a few seconds, and then plugging it back in, or, if everything else fails, by restarting the computer. After initial installation, the rule file will be activated automatically when the computer boots or when a Kinect device is plugged into any USB port.</p>"},{"location":"kinect/installation/intrinsically_calibrating_a_single_kinect/","title":"Intrinsically calibrating a single Kinect","text":"<p>Before a Kinect can be used as a 3D camera, its two cameras need to be calibrated. Calibration will calculate a 4x4 projection matrix to map the depth camera's image stream into 3D space as a 3D point cloud or triangulated surface, and a 4x4 projection matrix to map the color camera's image stream onto that point cloud or surface. After proper calibration, the virtual 3D objects captured by a Kinect will precisely match the original real objects in shape and size.</p> <p>The quickest way to get intrinsic calibration data for a Kinect is to download the factory calibration data that is stored in each Kinect's firmware. The Kinect package contains a utility for this purpose. In a terminal, run:</p> <pre><code>sudo KinectUtil getCalib &lt;Kinect index&gt;\n</code></pre> <p>where <code>&lt;Kinect index&gt;</code> is the index of the Kinect whose data you want to download. If you have only one Kinect connected to your computer, its index is 0. KinectUtil will download the data and copy it into a file that will later be found automatically by all Kinect applications. The quality of factory calibration is generally not as good as what can be achieved by custom calibration, see below, especially for Xbox 360 Kinects.</p> <p>Custom intrinsic calibration has two sub-procedures. The first, optional, one is to calculate per-pixel depth correction equations. Due to radial lens distortion in the IR pattern projector and the IR camera, a completely flat surface seen by the Kinect will not be reconstructed as a flat surface, but as a bowl shape. Depth correction will calculate a linear equation for each depth pixel that can rectify distance values. The second sub-procedure is to calculate 4x4 homogeneous depth unprojection and color projection matrices. The results of sub-procedure two depend on sub-procedure one, meaning that the second has to be performed every time after the first one is performed.</p>"},{"location":"kinect/installation/per_pixel_depth_correction/","title":"Per pixel depth correction","text":""},{"location":"kinect/installation/per_pixel_depth_correction/#per-pixel-depth-correction","title":"Per-pixel Depth Correction","text":"<p>Calculating per-pixel depth correction equations only requires a flat and ideally vertical surface large enough to fill the Kinect's field-of-view at larger distances (up to about 2 meters). These are the calibration procedure's steps:</p> <ol> <li> <p>Start RawKinectViewer for the Kinect to be calibrated.</p> </li> <li> <p>Create a \"Calibrate Depth Lens\" tool by binding it to two (2) buttons    (keyboard keys or mouse buttons). Here, we will use the keys \"1\" and    \"2\". Pressing the \"1\" key will capture the current depth image as a    calibration point, and pressing the \"2\" key will calculate per-pixel    depth correction equations after several depth images have been    captured.</p> </li> <li> <p>Move the Kinect such that it faces the flat surface orthogonally and    at close distance. The best approach is to position the Kinect to be    roughly orthogonal to the surface, and push it so close that the    entire depth image turns black (this happens at a distance of around    50cm). Then slowly pull the Kinect back until about half the pixels    have valid depth values. Then carefully rotate the Kinect until the    valid pixels are distributed evenly across the depth image, and then    pull back slightly further until most or all pixels have valid depth.</p> </li> <li> <p>Press the \"1\" key to capture the current depth image. This will    capture a number of frames, as indicated by the pop-up window. Do not    move the Kinect while the pop-up window stays on the screen.</p> </li> <li> <p>Move the Kinect further back, ideally until a sharp change in depth    mapping color. Carefully rotate the Kinect such that pixels of    different colors are evenly distributed, and adjust the distance    until there are approximately the same number of pixels of either    color.</p> </li> <li> <p>Repeat from step 4, until the distance from the Kinect to the flat    surface is larger than the largest distance you intend to capture    during use, or repeat until the Kinect is approximately 2m away from    the surface (the maximum practical capture distance).</p> </li> <li> <p>Press the \"2\" key to calculate per-pixel depth correction equations.    When the calculation is complete, the correction parameters will    automatically be written to a DepthCorrection-.dat    file in the Kinect package's configuration directory. The correction    calculation may fail if there are too many depth pixels that don't    have valid depth data, i.e., show up in black, in too many depth    images. If an error message appears after pressing \"2\", capture    additional depth images within the desired depth range as in step 4,    and press \"2\" again. <li> <p>Exit from RawKinectViewer.</p> </li>"},{"location":"kinect/installation/projection_matrix_calc/","title":"Projection matrix calc","text":""},{"location":"kinect/installation/projection_matrix_calc/#projection-matrix-calculation","title":"Projection matrix calculation","text":"<p>The new intrinsic calibration method uses a semi-transparent checkerboard calibration target. The idea is that the target looks like a checkerboard both to the depth and color cameras. The user presents the calibration target to the Kinect in a sequence of different poses, and manually matches a 2D distorted grid to the observed calibration target in the depth and color cameras' views using the <code>RawKinectViewer</code> utility. After a sufficient number of poses have been captured, <code>RawKinectViewer</code> calculates the two calibration matrices and stores them in a uniquely identified file for subsequent retrieval whenever a Kinect device is activated.</p> <p>The easiest and most precise way to build a semi-transparent checkerboard is to print a grid pattern onto a large sheet of paper, glue the entire sheet of paper onto an IR-transparent glass plate, cut precisely along all grid lines using a sharp knife and metal ruler, and then peel off every other paper square and remove any glue residue from the now transparent tiles. The grid should have an odd number of tiles in both directions such that all four corner tiles can stay opaque. Mark the center of the lower-left corner tile with a small dot (big enough to be visible in the Kinect's color image stream). This mark will be used to check for grid alignment during calibration.</p> <p>The number of tiles, and the size of each tile, can be configured in <code>RawKinectViewer</code>, but the default grid layout has 7 \\(\\times\\) 5 tiles of 3.5\" \\(\\times\\) 3.5\" each. This leads to an overall target size of 24.5\" \\(\\times\\) 17.5\", which is just the right size to cover the Kinect's full viewing range.</p> <p>The following is the intrinsic calibration procedure for a Kinect. There are two tutorial videos on YouTube:</p> <ul> <li>Video 1 explains steps 0 to 9 in  the following procedure.</li> <li>Video 2 explains step 10 in the  following procedure.</li> </ul>"},{"location":"kinect/installation/projection_matrix_calc/#steps","title":"Steps","text":"<ol> <li> <p>Prepare calibration target</p> </li> <li> <p>Start <code>RawKinectViewer</code> for the Kinect to be calibrated. If the calibration target does not have the default layout, specify the layout using the <code>-gridSize</code> and <code>-tileSize</code> command line options.</p> </li> <li> <p>Create a \"Draw Grids\" tool by binding it to six (6) buttons (keyboard keys or mouse buttons). Here, we will use the keys 1, W, 2, 3, 4, 5, in that order. This will create two green grids, one in the depth and one in the color image stream. The grid can be deformed by grabbing any grid intersection using the mouse and the 1 key, or translated by grabbing the dot in the center of the grid, or rotated around the center by grabbing the dot slightly outside the right edge of the grid, all using the 1 key.</p> </li> <li> <p>Place the calibration target in front of the Kinect at the next position and orientation. The target should be placed in a range of distances, starting from the near depth cutoff to the end of the desired tracking range, and in a variety of orientations from straight-on to about 45 degree angles. Calibration requires at least 4 calibration poses, but ideally a larger number to improve calibration quality.</p> </li> <li> <p>Collect an average depth frame by turning on the \"Average Frames\" button in the main menu. Wait until the depth image does not change anymore.</p> </li> <li> <p>Align both the depth and color grids grids to the observed grids. The dot in the center of the lower-left corner tile must roughly coincide with the mark in the lower-left corner tile of the calibration target to ensure that the grid is not flipped or rotated. The orientation of the depth and color stream grids will be very similar.</p> </li> <li> <p>Store the current calibration grids by pressing the 2 key.</p> </li> <li> <p>Turn off the \"Average Frames\" button in the main menu so that the depth image stream updates in real time again.</p> </li> <li> <p>Repeat from step 3 with the next calibration pose.</p> </li> <li> <p>After all calibration poses have been captured, calculate the intrinsic calibration by pressing the 4 key. This will print some diagnostic information to the terminal, and create the calibration file specific to the Kinect device in the Kinect package's configuration directory. The files are named <code>IntrinsicParameters-&lt;serial number&gt;.dat</code> and contain the two calibration matrices in binary format. They cannot be hand-edited.</p> </li> <li> <p>After calibration, use KinectViewer and a 3D measurement tool to ensure that the size and shape of the virtual calibration target as reconstructed by the Kinect matches the real calibration target. A good calibration will create a match to a few percent. Keep in mind that measurements inside KinectViewer will be reported in centimeters.</p> </li> </ol>"},{"location":"kinect/usage/","title":"Running the Kinect package","text":""},{"location":"kinect/usage/#step-1-run-binkinectutil-list-to-list-all-kinect-devices-connected-to-the-host-computer","title":"Step 1. Run ./bin/KinectUtil list to list all Kinect devices connected to the host computer.","text":""},{"location":"kinect/usage/#step-2a-run-sudo-binkinectutil-getcalib-to-download-factory-calibration-data-from-the-th-kinect-device-on-the-hosts-usb-bus-with-the-first-device-having-index-0-and-create-a-intrinsic-camera-calibration-file-for-that-device-in-the-kinect-packages-configuration-file-directory","title":"Step 2a. Run sudo ./bin/KinectUtil getCalib  to download  factory calibration data from the -th Kinect device on  the host's USB bus (with the first device having index 0) and create  a intrinsic camera calibration file for that device in the Kinect  package's configuration file directory.  <p>OR</p>","text":""},{"location":"kinect/usage/#step-2b-run-binrawkinectviewer-to-intrinsically-calibrate-the-th-kinect-device-on-the-hosts-usb-bus-with-the-first-device-having-index-0-using-a-semi-transparent-checkerboard-calibration-target-see-next-section-for-details-this-generates-a-binary-intrinsicparameters-dat-file-in-etckinect-where-is-the-unique-factory-assigned-serial-number-of-the-kinect-device-as-displayed-in-step-1","title":"Step 2b. Run ./bin/RawKinectViewer  to intrinsically calibrate  the -th Kinect device on the host's USB bus (with the  first device having index 0), using a semi-transparent checkerboard  calibration target (see next section for details). This generates a  binary IntrinsicParameters-.dat file in  /etc/Kinect-, where  is  the unique factory-assigned serial number of the Kinect device as  displayed in step 1. <p>Step 2 (a or b) only have to be performed once for each Kinect device, unless the resulting calibration proves unsatisfactory. Kinect devices typically retain their intrinsic calibration over time.</p>","text":""},{"location":"kinect/usage/#step-3-run-binkinectviewer-c-where-is-the-zero-based-index-of-the-kinect-device-to-use-if-there-are-multiple-ones","title":"Step 3 Run ./bin/KinectViewer -c , where  is the zero-based index of the Kinect device to use if there are multiple ones.","text":""},{"location":"kinect/usage/merging_3d_facades/","title":"Merging 3D Facades from Multiple Kinects","text":"<p>To merge the 3D reconstructions of multiple Kinect cameras, first intrinsically calibrate each one using <code>RawKinectViewer</code>, as described above and shown in the videos. Then extrinsically calibrate all Kinect devices with respect to an arbitrarily chosen world coordinate system.</p> <ol> <li> <p>Place calibration target into field of view of all Kinect devices to    be calibrated (use <code>RawKinectViewer</code> on each to check).</p> </li> <li> <p>For each Kinect device:</p> </li> </ol> <p>2a. Run <code>RawKinectViewer</code> and fit a grid to the calibration target's        image in the depth stream (it is not necessary to fit to the        color stream).</p> <p>2b. Save and unproject the grid to receive a list of 3D tie points on        the console.</p> <p>2c. Copy the tie points into a file <code>KinectPoints-&lt;serial number&gt;.csv</code>.</p> <ol> <li> <p>Create a file TargetPoints.csv, containing the 3D interior corner    positions of the calibration target, in some arbitrary right-handed    world coordinate system using an arbitrary unit of measurement, in    the same order as printed by <code>RawKinectViewer</code> (going from left to    right, and then bottom to top). See example target file below.</p> </li> <li> <p>For each Kinect device:</p> </li> </ol> <p>4a. Run the Kinect device's tie points and the target points through        AlignPoints:</p> <pre><code>   AlignPoints -OG KinectPoints-&lt;serial number&gt;.csv TargetPoints.csv\n</code></pre> <p>4b. Observe the mismatches between purple camera points and green        target points in AlignPoints' display. If the discrepancies are        too large, or there is no fit at all, repeat step two for the        Kinect device.</p> <p>4c. Paste the final best fitting transform displayed by AlignPoints        (everything after the \"Best transformation:\" header) into a        <code>ExtrinsicParameters-&lt;serial number&gt;.txt</code> file in the Kinect        package's configuration directory (<code>Kinect-&lt;version&gt;</code> in the Vrui        installation's <code>etc/</code> directory).</p> <ol> <li>Run <code>KinectViewer</code> on all Kinect devices. This will show all individual    3D video streams matching more or less seamlessly, depending on how    much care was taken during intrinsic and extrinsic calibration. Under    ideal circumstances, there should be no noticeable mismatches between    individual streams.</li> </ol> <p>Unlike intrinsic calibration, extrinsic calibration has to be repeated any time any of the calibrated Kinect devices are moved, even by very little. It is recommended to rigidly attach all Kinect devices to a sturdy frame before attempting precise extrinsic calibration in a production setting. If precision is paramount, it might even be necessary to repeat calibration periodically if nothing changed, to account for time-varying deformations inside the devices themselves.</p> <p>An example TargetPoints.csv file, for a 5x4 calibration grid with a tile size of 3\" (a 5x4 grid has 4x3 interior corners):</p> <pre><code>0, 0, 0\n3, 0, 0\n6, 0, 0\n9, 0, 0\n0, 3, 0\n3, 3, 0\n6, 3, 0\n9, 3, 0\n0, 6, 0\n3, 6, 0\n6, 6, 0\n9, 6, 0\n</code></pre> <p>The resulting world space will use whatever units were used to define the target points; in the example above, world space will use inches.</p>"},{"location":"kinect/usage/recording_3d_movies/","title":"Recording 3D Movies","text":"<p>KinectViewer now has a built-in recorder for 3D movies from one or more Kinect devices. When running a live view (using one or more <code>-c &lt;Kinect index&gt;</code> and/or <code>-p &lt;server host&gt; &lt;server port&gt;</code> command line arguments), the live 3D streams can be saved to a set of files by selecting the \"Save Streams\" main menu entry.</p> <p>KinectViewer will ask for a file name prefix, and then save the depth and color streams of all enabled Kinect devices to files <code>&lt;prefix&gt;-&lt;index&gt;.depth</code> and <code>&lt;prefix&gt;-&lt;index&gt;.color</code>. It will also record synchronized audio from the default capture device (selectable from the sound control panel) and save it to <code>&lt;prefix&gt;.wav</code>.</p> <p>Previously recorded 3D movies can be played back by running KinectViewer with one or more <code>-f &lt;file name prefix&gt;-&lt;index&gt;</code> arguments, one per saved 3D stream, and an optional <code>-s &lt;sound file name&gt;</code> argument to play back synchronized audio.</p>"},{"location":"vrui/","title":"VRUI","text":""},{"location":"vrui/installation/","title":"Getting started with VRUI","text":"<p>Follow these instructions to install VRUI with standard settings in a standard location, to simplify building and installing add-on packages and VRUI applications.</p> Heads up! <p>Angle brackets <code>&lt;&gt;</code> in commands below are placeholders, meaning that you have to replace everything between, and including, the angle brackets with some text that depends on your specific circumstances.</p> <p>For example, if your host has eight CPUs, instead of entering <code>-j&lt;number of CPUs&gt;</code> as part of some command, you would enter <code>-j8</code>.</p>"},{"location":"vrui/installation/#step-1-download-the-vrui-repository-from-github","title":"Step 1: Download the VRUI repository from GitHub","text":"<p>The VRUI code repository can be downloaded either by:</p> <ol> <li>downloading the zip file and unpacking it OR</li> <li>cloning the repository with <code>git clone</code></li> </ol> <p>Warning</p> <p>If you are unfamiliar with git and/or GitHub, you should probably go the zip file route.</p>"},{"location":"vrui/installation/#option-1-downloading-and-unpacking-a-zip-file-from-github","title":"Option 1: Downloading and unpacking a zip file from GitHub","text":"<p>On the VRUI repository's main page, click on the green \"&lt;&gt; Code\" button, and then click on \"Download ZIP\" in the menu that pops up in response.</p> <p>Depending on your browser settings, you may be asked where to store the file being downloaded, or it might be stored in a default location, such as your <code>Downloads</code> directory. Take note of what the zip file is called and where it is stored.</p> <p>Then, once the file is completely downloaded, enter the following multiple lines into a terminal window:</p> <pre><code>cd ~\nmkdir src\ncd src\n</code></pre> <pre><code>cd ~/src\n</code></pre> <p>Then enter into the same terminal window:</p> <pre><code>unzip &lt;path to downloaded zip file&gt;\n</code></pre> <p>Replace <code>&lt;path to downloaded zip file&gt;</code> with the full path to the zip file, for example <code>~/Downloads/vrui-main.zip</code>.</p> <p>Finally, check for the name of your new VRUI directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>VRUI-main</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;VRUI directory&gt;\n</code></pre> <p>where you replace <code>&lt;VRUI directory&gt;</code> with the name of the directory where you cloned/unpacked the VRUI in the previous step, as printed by <code>ls</code>.</p>"},{"location":"vrui/installation/#option-2-clone-the-repository-from-github","title":"Option 2: Clone the repository from GitHub","text":"<p>First, create a directory in your terminal where the VRUI code will live:</p> <pre><code>cd ~\nmkdir src\ncd src\n</code></pre> <p>Then, navigate to this directory:</p> <pre><code>cd ~/src\n</code></pre> <p>Now, we can clone the repository from GitHub:</p> <pre><code>git clone https://github.com/vrui-vr/vrui.git\n</code></pre> <p>Finally, check for the name of your new VRUI directory by entering:</p> <pre><code>ls\n</code></pre> <p>which will list all files in the <code>src</code> directory, which should include a new directory called <code>VRUI</code>. Take note of this name, and then enter into that directory by typing this command into the terminal window:</p> <pre><code>cd &lt;VRUI directory&gt;\n</code></pre> <p>where you replace <code>&lt;VRUI directory&gt;</code> with the name of the directory where you cloned/unpacked the VRUI in the previous step, as printed by <code>ls</code>.</p>"},{"location":"vrui/installation/#step-2-install-prerequisite-packages","title":"Step 2: Install Prerequisite Packages","text":"<p>VRUI uses a relatively large set of system-provided packages to implement its functionality. Some of these are essential, some are optional, some are very optional. See the README file for the complete list of prerequisite system packages. Ideally, you should install the full set of packages to unlock all of VRUI's functionality.</p> <p>To simplify installation, we provide a shell script that tries to determine the Linux distribution installed on the host, and tries to download and install prerequisite packages automatically. To run that script, enter into the same terminal window:</p> <pre><code>bash ./InstallPrerequisites.sh\n</code></pre> <p>which  will ask you to enter your user account's password when necessary, and will print a green completion message at the end if at least all required system packages were successfully installed. Do not proceed if the script ends with a red error message.</p>"},{"location":"vrui/installation/#step-3-build-vrui","title":"Step 3: Build VRUI","text":"<p>To build VRUI, enter into the same terminal window:</p> <pre><code>make\n</code></pre> Tip <p>You can speed up the build process if your host has multiple CPUs or CPU cores. Instead of the above, enter into the same terminal:</p> <p><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt; -j&lt;number of cpus&gt;</code></p> <p>again replacing <code>&lt;Vrui build system location&gt;</code> with the location of Vrui's build system on your host, and replacing <code>&lt;number of cpus&gt;</code> with the number of CPUs or CPU cores on your host, say <code>-j8</code> if you have eight cores. Note that there is no space between the <code>-j</code> and the number of cores.</p> <p>Using <code>-j$(nproc)</code> (exactly as written) will tell your computer to figure out how many cores it has.</p> <p>Building VRUI might take a few minutes, and will print lots of output to the terminal window. Be patient, and, once it's done, check that there were no error messages. The quickest way to check whether VRUI built successfully is to run <code>make</code> a second time:</p> <pre><code>make\n</code></pre> <p>If everything went well the first time, it will print:</p> <pre><code>make: Nothing to be done for 'all'.\n</code></pre> <p>This build process will prepare VRUI to be installed inside the <code>/usr/local</code> directory tree, which is the traditional place for software installed from source. Scroll back through the output from <code>make</code> and locate the following section towards the beginning:</p> <pre><code>---- VRUI installation configuration ----\nRoot installation directory               : /usr/local\nHeader installation root directory        : /usr/local/include/VRUI-13.1\nLibrary installation root directory       : /usr/local/lib64/VRUI-13.1\nExecutable installation directory         : /usr/local/bin\nPlug-in installation root directory       : /usr/local/lib64/VRUI-13.1\nConfiguration file installation directory : /usr/local/etc/VRUI-13.1\nShared file installation root directory   : /usr/local/share/VRUI-13.1\nMakefile fragment installation directory  : /usr/local/share/VRUI-13.1\nBuild system installation directory       : /usr/local/share/VRUI-13.1/make\npkg-config metafile installation directory: /usr/local/lib64/pkgconfig\nDocumentation installation directory      : /usr/local/share/doc/VRUI-13.1\n</code></pre> <p>(Your output may look slightly different.)</p> <p>The most important of those lines is this one:</p> <pre><code>Build system installation directory       : /usr/local/share/VRUI-13.1/make\n</code></pre> <p>showing the location of VRUI's build system, and this is the location you will need to use to build add-on packages or VRUI applications later. Take note of the precise location, in this example <code>/usr/local/share/VRUI-13.1/make</code>.</p>"},{"location":"vrui/installation/#step-4-install-vrui","title":"Step 4: Install VRUI","text":"<p>After building VRUI successfully, you can install it in the configured location by entering the following into the same terminal window:</p> <pre><code>sudo make install\n</code></pre> <p>which will ask you for your user account's password to install VRUI in a system location, and then install it. This should be quick. After the command completes, check that there were no errors.</p> <p>If there were no errors, you are done! \ud83c\udf89</p>"},{"location":"vrui/installation/#optional-build-vruis-example-applications","title":"Optional: Build VRUI's Example Applications","text":"<p>VRUI comes packaged with a few example applications demonstrating how to create VRUI-based VR applications. You can build these now to get a feel for how to build other VRUI applications later, or to test whether VRUI is working properly.</p>"},{"location":"vrui/installation/#step-1-enter-the-exampleprograms-directory","title":"Step 1: Enter The ExamplePrograms Directory","text":"<p>Enter into the same terminal window:</p> <pre><code>cd ExamplePrograms\n</code></pre>"},{"location":"vrui/installation/#step-2-build-the-example-applications","title":"Step 2: Build The Example Applications","text":"<p>Run <code>make</code> to build the example programs. During building VRUI itself, the build procedure automatically configured the <code>makefile</code> in the <code>ExamplePrograms</code> directory to find the VRUI installation. But to practice for building other VRUI applications later, you should still pass the location of VRUI's build system, mentioned above, to <code>make</code>. Enter into the same terminal window:</p> <pre><code>make VRUI_MAKEDIR=&lt;VRUI build system location&gt;\n</code></pre> <p>where you replace <code>&lt;VRUI build system location&gt;</code> with the location of VRUI's build system on your host, as described in the previous section.</p> <p>Example</p> <p>Your command will look something like this:</p> <p><code>sh make VRUI_MAKEDIR=/usr/local/share/VRUI-13.1/make</code></p> Tip <p>You can speed up the build process if your host has multiple CPUs or CPU cores. Instead of the above, enter into the same terminal:</p> <p><code>make VRUI_MAKEDIR=&lt;Vrui build system location&gt; -j&lt;number of cpus&gt;</code></p> <p>again replacing <code>&lt;Vrui build system location&gt;</code> with the location of Vrui's build system on your host, and replacing <code>&lt;number of cpus&gt;</code> with the number of CPUs or CPU cores on your host, say <code>-j8</code> if you have eight cores. Note that there is no space between the <code>-j</code> and the number of cores.</p> <p>Using <code>-j$(nproc)</code> (exactly as written) will tell your computer to figure out how many cores it has.</p> <p>If there were no errors, you should now see a new <code>bin</code> directory in the <code>ExamplePrograms</code> directory.</p>"},{"location":"vrui/installation/#step-3-run-an-example-application","title":"Step 3: Run An Example Application","text":"<p>To run one of the example applications, enter into the same terminal window:</p> <pre><code>./bin/ShowEarthModel\n</code></pre> <p>This should open a new window on your desktop, titled \"ShowEarthModel,\" and display a spinning globe. Congratulations, your VRUI installation is complete and working! To quit the example program, either press the Esc key, or simply close the application's window.</p>"}]}